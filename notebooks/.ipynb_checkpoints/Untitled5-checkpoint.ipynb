{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3938809a-38df-41cf-95de-f0761e2ccbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "AWS_ACCESS_KEY = \"minioadmin\"\n",
    "AWS_SECRET_KEY = \"minioadmin\"\n",
    "AWS_S3_ENDPOINT = \"http://minio_server:9000\"\n",
    "WAREHOUSE = \"s3a://gold/\" \n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"Lakehouse-Iceberg-TrainModel\")  \n",
    "    .set('spark.jars.packages',\n",
    "         'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1,'\n",
    "         'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.67.0,'\n",
    "         'org.apache.hadoop:hadoop-aws:3.3.4,'\n",
    "         'com.amazonaws:aws-java-sdk-bundle:1.12.300')\n",
    "    .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "    .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.endpoint\", AWS_S3_ENDPOINT)\n",
    "    .set(\"spark.sql.catalog.nessie.s3.access-key\", AWS_ACCESS_KEY)\n",
    "    .set(\"spark.sql.catalog.nessie.s3.secret-key\", AWS_SECRET_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf) \n",
    "    .config(\"spark.driver.memory\", \"4g\") \n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105d4ecd-7d4a-446b-8b59-1b7ffc51941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = spark.table(\"nessie.fact_order\")\n",
    "df_customer = spark.table(\"nessie.dim_customer\")\n",
    "df_product = spark.table(\"nessie.dim_product\")\n",
    "df_time = spark.table(\"nessie.dim_time\")\n",
    "df_location = spark.table(\"nessie.dim_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f97ced0-bfab-4433-97bd-5fa2b8955071",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT  \n",
    "    f.time_id,\n",
    "    f.customer_id,\n",
    "    f.product_id,\n",
    "    f.location_id,\n",
    "    f.purchase_price_per_unit,\n",
    "    f.quantity,\n",
    "    f.total_price,\n",
    "\n",
    "    -- Dim_time\n",
    "    t.order_date,\n",
    "    t.year,\n",
    "    t.month,\n",
    "    t.day,\n",
    "    t.quarter,\n",
    "    t.weekday_name,\n",
    "\n",
    "    -- Dim_customer\n",
    "    c.age_group,\n",
    "    c.gender,\n",
    "    c.education,\n",
    "    c.income,\n",
    "    c.race,\n",
    "    c.state,\n",
    "\n",
    "    -- Dim_product\n",
    "    p.product_title,\n",
    "    p.product_category,\n",
    "\n",
    "    -- Dim_location\n",
    "    l.state_code,\n",
    "    l.state_name,\n",
    "    l.region\n",
    "\n",
    "FROM nessie.fact_order AS f\n",
    "LEFT JOIN nessie.dim_time AS t ON f.time_id = t.time_id\n",
    "LEFT JOIN nessie.dim_customer AS c ON f.customer_id = c.customer_id\n",
    "LEFT JOIN nessie.dim_product AS p ON f.product_id = p.product_id\n",
    "LEFT JOIN nessie.dim_location AS l ON f.location_id = l.location_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3deb046f-023a-42fe-ae24-d91bc914fefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>purchase_price_per_unit</th>\n",
       "      <th>quantity</th>\n",
       "      <th>total_price</th>\n",
       "      <th>order_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>race</th>\n",
       "      <th>state</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_name</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439</td>\n",
       "      <td>R_1jZkLNE1JdtyVpH</td>\n",
       "      <td>000217653X</td>\n",
       "      <td>44</td>\n",
       "      <td>29.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Female</td>\n",
       "      <td>High school diploma or GED</td>\n",
       "      <td>Less than $25,000</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Florida</td>\n",
       "      <td>THE DINAH'S CUPBOARD COOK BOOK: Recipes and Me...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>FL</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439</td>\n",
       "      <td>R_1jZkLNE1JdtyVpH</td>\n",
       "      <td>000217653X</td>\n",
       "      <td>39</td>\n",
       "      <td>13.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.55</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Female</td>\n",
       "      <td>High school diploma or GED</td>\n",
       "      <td>Less than $25,000</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Florida</td>\n",
       "      <td>THE DINAH'S CUPBOARD COOK BOOK: Recipes and Me...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>444</td>\n",
       "      <td>R_3qIPMah81MezsJn</td>\n",
       "      <td>0007137508</td>\n",
       "      <td>33</td>\n",
       "      <td>19.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.95</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>$50,000 - $74,999</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Wellington: The Iron Duke</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>TN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428</td>\n",
       "      <td>R_vD2O13NgdnWBXMt</td>\n",
       "      <td>0007302622</td>\n",
       "      <td>4</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>2019-08-10</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Female</td>\n",
       "      <td>Graduate or professional degree (MA, MS, MBA, ...</td>\n",
       "      <td>$50,000 - $74,999</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Duck in the Truck</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1573</td>\n",
       "      <td>R_1QsZS0nI2sw5gl5</td>\n",
       "      <td>000745287X</td>\n",
       "      <td>41</td>\n",
       "      <td>14.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.96</td>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Graduate or professional degree (MA, MS, MBA, ...</td>\n",
       "      <td>$150,000 or more</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Sharpe's Regiment: Richard Sharpe and the Inva...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>GA</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1815</td>\n",
       "      <td>R_2aldwxmUZox7Yfd</td>\n",
       "      <td>0007483791</td>\n",
       "      <td>16</td>\n",
       "      <td>10.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.84</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Graduate or professional degree (MA, MS, MBA, ...</td>\n",
       "      <td>$150,000 or more</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>California</td>\n",
       "      <td>Deep Time</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>R_3GD1CL4OyjglmbZ</td>\n",
       "      <td>0007510837</td>\n",
       "      <td>36</td>\n",
       "      <td>24.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.04</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Female</td>\n",
       "      <td>High school diploma or GED</td>\n",
       "      <td>$25,000 - $49,999</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Collins German Dictionary Complete and Unabrid...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>PA</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>328</td>\n",
       "      <td>R_3Pc1ZZfNy58AvgE</td>\n",
       "      <td>0007544790</td>\n",
       "      <td>16</td>\n",
       "      <td>18.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.72</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>$100,000 - $149,999</td>\n",
       "      <td>Other</td>\n",
       "      <td>California</td>\n",
       "      <td>My Virgin Kitchen: Delicious recipes you can m...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>354</td>\n",
       "      <td>R_27Nf8ImFlWu3J9O</td>\n",
       "      <td>000756032X</td>\n",
       "      <td>4</td>\n",
       "      <td>9.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.99</td>\n",
       "      <td>2018-06-20</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>High school diploma or GED</td>\n",
       "      <td>Less than $25,000</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>California</td>\n",
       "      <td>Born into the Children of God: My life in a re...</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1201</td>\n",
       "      <td>R_3Pp1HTLxoglta9u</td>\n",
       "      <td>0008100713</td>\n",
       "      <td>32</td>\n",
       "      <td>23.08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.08</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Graduate or professional degree (MA, MS, MBA, ...</td>\n",
       "      <td>$75,000 - $99,999</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Well Gardened Mind</td>\n",
       "      <td>ABIS_BOOK</td>\n",
       "      <td>OH</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id        customer_id  product_id  location_id  \\\n",
       "0      439  R_1jZkLNE1JdtyVpH  000217653X           44   \n",
       "1      439  R_1jZkLNE1JdtyVpH  000217653X           39   \n",
       "2      444  R_3qIPMah81MezsJn  0007137508           33   \n",
       "3      428  R_vD2O13NgdnWBXMt  0007302622            4   \n",
       "4     1573  R_1QsZS0nI2sw5gl5  000745287X           41   \n",
       "5     1815  R_2aldwxmUZox7Yfd  0007483791           16   \n",
       "6       23  R_3GD1CL4OyjglmbZ  0007510837           36   \n",
       "7      328  R_3Pc1ZZfNy58AvgE  0007544790           16   \n",
       "8      354  R_27Nf8ImFlWu3J9O  000756032X            4   \n",
       "9     1201  R_3Pp1HTLxoglta9u  0008100713           32   \n",
       "\n",
       "   purchase_price_per_unit  quantity  total_price  order_date  year  month  \\\n",
       "0                    29.99       1.0        29.99  2020-09-16  2020      9   \n",
       "1                    13.55       1.0        13.55  2020-09-16  2020      9   \n",
       "2                    19.95       1.0        19.95  2022-12-05  2022     12   \n",
       "3                    13.25       1.0        13.25  2019-08-10  2019      8   \n",
       "4                    14.96       1.0        14.96  2022-06-27  2022      6   \n",
       "5                    10.84       1.0        10.84  2018-03-21  2018      3   \n",
       "6                    24.04       1.0        24.04  2020-01-21  2020      1   \n",
       "7                    18.72       1.0        18.72  2019-07-23  2019      7   \n",
       "8                     9.99       1.0         9.99  2018-06-20  2018      6   \n",
       "9                    23.08       1.0        23.08  2022-06-08  2022      6   \n",
       "\n",
       "   ...  gender                                          education  \\\n",
       "0  ...  Female                         High school diploma or GED   \n",
       "1  ...  Female                         High school diploma or GED   \n",
       "2  ...    Male                                  Bachelor's degree   \n",
       "3  ...  Female  Graduate or professional degree (MA, MS, MBA, ...   \n",
       "4  ...    Male  Graduate or professional degree (MA, MS, MBA, ...   \n",
       "5  ...    Male  Graduate or professional degree (MA, MS, MBA, ...   \n",
       "6  ...  Female                         High school diploma or GED   \n",
       "7  ...    Male                                  Bachelor's degree   \n",
       "8  ...    Male                         High school diploma or GED   \n",
       "9  ...    Male  Graduate or professional degree (MA, MS, MBA, ...   \n",
       "\n",
       "                income                race         state  \\\n",
       "0    Less than $25,000  White or Caucasian       Florida   \n",
       "1    Less than $25,000  White or Caucasian       Florida   \n",
       "2    $50,000 - $74,999  White or Caucasian     Tennessee   \n",
       "3    $50,000 - $74,999  White or Caucasian    New Jersey   \n",
       "4     $150,000 or more  White or Caucasian       Georgia   \n",
       "5     $150,000 or more  White or Caucasian    California   \n",
       "6    $25,000 - $49,999  White or Caucasian  Pennsylvania   \n",
       "7  $100,000 - $149,999               Other    California   \n",
       "8    Less than $25,000  White or Caucasian    California   \n",
       "9    $75,000 - $99,999  White or Caucasian          Ohio   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  THE DINAH'S CUPBOARD COOK BOOK: Recipes and Me...        ABIS_BOOK   \n",
       "1  THE DINAH'S CUPBOARD COOK BOOK: Recipes and Me...        ABIS_BOOK   \n",
       "2                          Wellington: The Iron Duke        ABIS_BOOK   \n",
       "3                                  Duck in the Truck        ABIS_BOOK   \n",
       "4  Sharpe's Regiment: Richard Sharpe and the Inva...        ABIS_BOOK   \n",
       "5                                          Deep Time        ABIS_BOOK   \n",
       "6  Collins German Dictionary Complete and Unabrid...        ABIS_BOOK   \n",
       "7  My Virgin Kitchen: Delicious recipes you can m...        ABIS_BOOK   \n",
       "8  Born into the Children of God: My life in a re...        ABIS_BOOK   \n",
       "9                                 Well Gardened Mind        ABIS_BOOK   \n",
       "\n",
       "  state_code  state_name     region  \n",
       "0         FL     Unknown    Unknown  \n",
       "1         TX       Texas      South  \n",
       "2         TN     Unknown    Unknown  \n",
       "3         NJ  New Jersey  Northeast  \n",
       "4         GA     Unknown    Unknown  \n",
       "5         CA  California       West  \n",
       "6         PA     Unknown    Unknown  \n",
       "7         CA  California       West  \n",
       "8         NJ  New Jersey  Northeast  \n",
       "9         OH     Unknown    Unknown  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_full = spark.sql(query)\n",
    "df_fact_full.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a759118-12db-41b0-9692-7e1832c1540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_id: long (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- purchase_price_per_unit: double (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekday_name: string (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hiển thị schema sau khi làm sạch\n",
    "df_fact_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc8cbed-990b-41da-8085-2e305aae0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kích thước dữ liệu: (1675015, 24)\n"
     ]
    }
   ],
   "source": [
    "# Đếm số dòng\n",
    "num_rows =df_fact_full.count()\n",
    "# Đếm số cột\n",
    "num_cols = len(df_fact_full.columns)\n",
    "print(f\"\\nKích thước dữ liệu: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13764286-81ac-4f50-a72d-4dddd99d59d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 167 | Test: 2689\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Tính amount và tổng hợp feature trong 1 groupBy\n",
    "# =====================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_fact_full = df_fact_full.withColumn('amount', F.col('purchase_price_per_unit') * F.col('quantity'))\n",
    "\n",
    "# Most frequent product_category\n",
    "df_cat = df_fact_full.groupBy(\"customer_id\", \"product_category\").agg(F.count(\"*\").alias(\"cnt\"))\n",
    "w_cat = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"cnt\"))\n",
    "df_cat_rank = df_cat.withColumn(\"rank\", F.row_number().over(w_cat))\n",
    "df_most_freq_category = df_cat_rank.filter(F.col(\"rank\")==1)\\\n",
    "                                  .select(\"customer_id\", F.col(\"product_category\").alias(\"most_freq_category\"))\n",
    "\n",
    "# Numeric features\n",
    "df_numeric = df_fact_full.groupBy(\"customer_id\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_spend\"),\n",
    "    F.count(\"amount\").alias(\"n_orders\"),\n",
    "    F.avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    F.stddev(\"amount\").alias(\"std_order_value\"),\n",
    "    (F.sum(\"quantity\") / F.countDistinct(\"time_id\")).alias(\"avg_items_per_order\"),\n",
    "    F.countDistinct(\"time_id\").alias(\"total_orders\"),\n",
    "    F.min(\"year\").alias(\"first_year\"),\n",
    "    F.max(\"year\").alias(\"last_year\")\n",
    ").withColumn(\"years_active\", (F.col(\"last_year\") - F.col(\"first_year\") + 1))\\\n",
    " .withColumn(\"years_active\", F.when(F.col(\"years_active\") <= 0, 1).otherwise(F.col(\"years_active\")))\\\n",
    " .fillna({'std_order_value': 0})\n",
    "\n",
    "# Merge features + demographics\n",
    "demographics_cols = ['gender','education','income','state']\n",
    "df_demo = df_fact_full.select('customer_id', *demographics_cols).dropDuplicates(['customer_id'])\n",
    "\n",
    "df_features = df_numeric.join(df_most_freq_category, on='customer_id', how='left')\\\n",
    "                        .join(df_demo, on='customer_id', how='left')\n",
    "\n",
    "# =====================================================\n",
    "# tạo target: next_year_orders\n",
    "# =====================================================\n",
    "df_target = df_fact_full.groupBy('customer_id','year').agg(F.count(\"*\").alias(\"orders_per_year\"))\n",
    "w = Window.partitionBy(\"customer_id\").orderBy(\"year\")\n",
    "df_target = df_target.withColumn(\"next_year_orders\", F.lead(\"orders_per_year\",1).over(w))\n",
    "df_target_max = df_target.groupBy(\"customer_id\").agg(F.max(\"next_year_orders\").alias(\"next_year_orders\"))\n",
    "\n",
    "df_final = df_features.join(df_target_max, on='customer_id', how='left').fillna({'next_year_orders':0})\n",
    "\n",
    "# =====================================================\n",
    "# Chuẩn bị features & target\n",
    "# =====================================================\n",
    "numeric_features = ['total_spend','n_orders','avg_order_value','std_order_value',\n",
    "                    'avg_items_per_order','total_orders','years_active']\n",
    "categorical_features = ['most_freq_category','gender','education','income','state']\n",
    "target = 'next_year_orders'\n",
    "\n",
    "# StringIndexer + OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in categorical_features]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_ohe\") for c in categorical_features]\n",
    "\n",
    "# VectorAssembler + StandardScaler\n",
    "assembler = VectorAssembler(inputCols=numeric_features + [c+\"_ohe\" for c in categorical_features],\n",
    "                            outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
    "\n",
    "# =====================================================\n",
    "# Chia train/test\n",
    "# =====================================================\n",
    "train_df = df_final.filter(F.col('last_year') <= 2021)\n",
    "test_df  = df_final.filter(F.col('last_year') == 2022)\n",
    "print(f\"Train: {train_df.count()} | Test: {test_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5fab6b-ee44-4a60-a29c-b421c8c11f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train sample ===\n",
      "+-----------------+------------------+--------+------------------+------------------+-------------------+------------+----------+---------+------------+-------------------------+------+--------------------------------------------------------------------+-----------------+-------------+----------------+\n",
      "|customer_id      |total_spend       |n_orders|avg_order_value   |std_order_value   |avg_items_per_order|total_orders|first_year|last_year|years_active|most_freq_category       |gender|education                                                           |income           |state        |next_year_orders|\n",
      "+-----------------+------------------+--------+------------------+------------------+-------------------+------------+----------+---------+------------+-------------------------+------+--------------------------------------------------------------------+-----------------+-------------+----------------+\n",
      "|R_2qEEh58vzt5peUq|2900.7            |108     |26.85833333333333 |55.7076340596454  |3.1020408163265305 |49          |2019      |2021     |3           |ABIS_BOOK                |Female|Prefer not to say                                                   |$25,000 - $49,999|New York     |54              |\n",
      "|R_3psrxBJAfpUyn3d|467.71000000000004|24      |19.487916666666667|16.414307910072438|1.5                |16          |2018      |2021     |4           |ABIS_BOOK                |Male  |High school diploma or GED                                          |$25,000 - $49,999|Arizona      |5               |\n",
      "|R_2ve3990Y7TsELdq|970.8299999999999 |49      |19.81285714285714 |14.596572799804754|2.1739130434782608 |23          |2018      |2021     |4           |PET_PEST_CONTROL         |Female|High school diploma or GED                                          |$75,000 - $99,999|Mississippi  |15              |\n",
      "|R_3khq81upkg41nqx|489.65999999999997|25      |19.586399999999998|13.604194475724514|2.0833333333333335 |12          |2019      |2021     |3           |TOY_FIGURE               |Female|High school diploma or GED                                          |$75,000 - $99,999|Georgia      |10              |\n",
      "|R_22PN2HxbOVDaT5t|683.75            |15      |45.583333333333336|74.9921891805756  |6.0                |3           |2018      |2019     |2           |ELECTRONIC_CABLE         |Male  |Bachelor's degree                                                   |$75,000 - $99,999|New York     |10              |\n",
      "|R_1Cf4l6dtpfbfIwm|181.23000000000002|14      |12.945000000000002|11.213398407537023|1.75               |8           |2020      |2021     |2           |TOY_FIGURE               |Female|Bachelor's degree                                                   |$50,000 - $74,999|Arizona      |10              |\n",
      "|R_1geFu1kcfEuAtA7|355.23            |16      |22.201875         |25.163282832664475|1.7777777777777777 |9           |2018      |2021     |4           |PIERCING_JEWELRY         |Male  |Bachelor's degree                                                   |$50,000 - $74,999|New Hampshire|12              |\n",
      "|R_3GlmpDF7aCOi9wA|4309.26           |119     |36.21226890756303 |45.96663175216279 |2.911111111111111  |45          |2018      |2021     |4           |DAIRY_BASED_DRINK        |Female|High school diploma or GED                                          |Less than $25,000|New Mexico   |65              |\n",
      "|R_33kXOnRQaZX4laR|511.77            |16      |31.985625         |30.881759010511903|1.2142857142857142 |14          |2018      |2020     |3           |PROTEIN_SUPPLEMENT_POWDER|Male  |Graduate or professional degree (MA, MS, MBA, PhD, JD, MD, DDS, etc)|$25,000 - $49,999|Ohio         |6               |\n",
      "|R_12bTyDTgHUIoMUQ|735.6700000000001 |37      |19.882972972972976|25.67898296108933 |2.0                |20          |2018      |2021     |4           |SKIN_MOISTURIZER         |Female|High school diploma or GED                                          |$25,000 - $49,999|Georgia      |7               |\n",
      "+-----------------+------------------+--------+------------------+------------------+-------------------+------------+----------+---------+------------+-------------------------+------+--------------------------------------------------------------------+-----------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In 10 dòng đầu train\n",
    "print(\"=== Train sample ===\")\n",
    "train_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3d9cd-e277-47a3-b22c-b049e95ecc0f",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a4bb04-56c2-4c09-b326-e008f4d8ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Model:\n",
      " - numTrees: 50\n",
      " - maxDepth: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --------------------------\n",
    "# Pipeline mô hình Random Forest\n",
    "# --------------------------\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=target,\n",
    "    featuresCol=\"scaledFeatures\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, rf])\n",
    "\n",
    "# --------------------------\n",
    "# Tập siêu tham số cần thử\n",
    "# --------------------------\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [50, 100])\n",
    "    .addGrid(rf.maxDepth, [10, 12])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# TrainValidationSplit\n",
    "# --------------------------\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=4\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Huấn luyện mô hình\n",
    "# --------------------------\n",
    "rf_tvs_model = tvs.fit(train_df)\n",
    "\n",
    "# --------------------------\n",
    "# In thông tin mô hình tốt nhất\n",
    "# --------------------------\n",
    "best_rf = rf_tvs_model.bestModel.stages[-1]\n",
    "print(\"Best Random Forest Model:\")\n",
    "print(\" - numTrees:\", best_rf.getNumTrees)\n",
    "print(\" - maxDepth:\", best_rf.getMaxDepth())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e1f502-51c6-4469-84e6-c89089bdbb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Train: 0.9539\n",
      "R2 Test : 0.2481\n",
      "----------\n",
      "MAE Train: 3.0962\n",
      "MAE Test : 48.1052\n",
      "----------\n",
      "RMSE Train: 6.2239\n",
      "RMSE Test : 105.6282\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Đánh giá mô hình\n",
    "# --------------------------\n",
    "train_pred = rf_tvs_model.bestModel.transform(train_df)\n",
    "test_pred  = rf_tvs_model.bestModel.transform(test_df)\n",
    "\n",
    "metrics = ['r2', 'mae', 'rmse']\n",
    "for metric in metrics:\n",
    "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=metric)\n",
    "    print(f\"{metric.upper()} Train:\", round(evaluator.evaluate(train_pred), 4))\n",
    "    print(f\"{metric.upper()} Test :\", round(evaluator.evaluate(test_pred), 4))\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eef4f4-deba-4994-9894-fd478a1230e0",
   "metadata": {},
   "source": [
    "## xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee4aa224-7ddf-4b2d-b981-d5076ef6ed91",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "most_freq_category_ohe does not exist. Available: customer_id, total_spend, n_orders, avg_order_value, std_order_value, avg_items_per_order, total_orders, first_year, last_year, years_active, most_freq_category, gender, education, income, state, next_year_orders, TrainValidationSplit_cc65fb4a4e7a_rand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 57\u001b[0m\n\u001b[1;32m     45\u001b[0m tvs \u001b[38;5;241m=\u001b[39m TrainValidationSplit(\n\u001b[1;32m     46\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb_pipeline,\n\u001b[1;32m     47\u001b[0m     estimatorParamMaps\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Huấn luyện mô hình\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m xgb_tvs_model \u001b[38;5;241m=\u001b[39m \u001b[43mtvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# In thông tin mô hình tốt nhất\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[1;32m     62\u001b[0m best_xgb \u001b[38;5;241m=\u001b[39m xgb_tvs_model\u001b[38;5;241m.\u001b[39mbestModel\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1462\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numModels))\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1462\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numModels))\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py:132\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[1;32m    131\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[0;32m--> 132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: most_freq_category_ohe does not exist. Available: customer_id, total_spend, n_orders, avg_order_value, std_order_value, avg_items_per_order, total_orders, first_year, last_year, years_active, most_freq_category, gender, education, income, state, next_year_orders, TrainValidationSplit_cc65fb4a4e7a_rand"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# --------------------------\n",
    "# XGBoost Spark phân tán\n",
    "# --------------------------\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"scaledFeatures\",\n",
    "    label_col=target,\n",
    "    num_workers=1,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    objective=\"reg:squarederror\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Pipeline\n",
    "# --------------------------\n",
    "xgb_pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, xgb])\n",
    "# --------------------------\n",
    "# Grid Search / TrainValidationSplit\n",
    "# --------------------------\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(xgb.max_depth, [4, 6])\n",
    "    .addGrid(xgb.n_estimators, [50, 100])\n",
    "    .addGrid(xgb.learning_rate, [0.05, 0.1])\n",
    "    .addGrid(xgb.subsample, [0.7, 0.8])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=xgb_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Huấn luyện mô hình\n",
    "# --------------------------\n",
    "xgb_tvs_model = tvs.fit(train_df)\n",
    "\n",
    "# --------------------------\n",
    "# In thông tin mô hình tốt nhất\n",
    "# --------------------------\n",
    "best_xgb = xgb_tvs_model.bestModel.stages[-1]\n",
    "print(\"Best XGBoost Model:\")\n",
    "print(\" - maxDepth:\", best_xgb.getMaxDepth())\n",
    "print(\" - n_estimators:\", best_xgb.getNEstimators())\n",
    "print(\" - learning_rate:\", best_xgb.getLearningRate())\n",
    "print(\" - subsample:\", best_xgb.getSubsample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a07fe-618a-4c56-a11e-f6a57f981ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Dự đoán & đánh giá\n",
    "# --------------------------\n",
    "train_pred = xgb_tvs_model.bestModel.transform(train_df)\n",
    "test_pred  = xgb_tvs_model.bestModel.transform(test_df)\n",
    "\n",
    "metrics = ['r2', 'mae', 'rmse']\n",
    "for metric in metrics:\n",
    "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=metric)\n",
    "    print(f\"{metric.upper()} Train:\", round(evaluator.evaluate(train_pred), 4))\n",
    "    print(f\"{metric.upper()} Test :\", round(evaluator.evaluate(test_pred), 4))\n",
    "    print(\"----------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
