version: "3.8"

services:
  # ---------------- Spark Cluster ----------------
  spark-master:
    image: bitnami/spark:3
    container_name: spark-master
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - 7077:7077    # Spark master port
      - 8080:8080    # Spark master web UI
    networks:
      - iceberg_env

  spark_worker_1:
    image: bitnami/spark:3
    container_name: spark_worker_1
    environment:
      SPARK_MODE: worker
      SPARK_MASTER: spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - iceberg_env

  spark_worker_2:
    image: bitnami/spark:3
    container_name: spark_worker_2
    environment:
      SPARK_MODE: worker
      SPARK_MASTER: spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - iceberg_env

  spark_notebook:
    image: jupyter/pyspark-notebook
    container_name: notebook
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports: 
      - 8888:8888   # Jupyter Notebook UI
    volumes:
      - ./notebooks:/home/jovyan/notebooks
      - ./mlflow/mlruns:/mlflow/mlruns
    env_file: .env
    networks:
      - iceberg_env

  # ---------------- MinIO ----------------
  minio_server:
    image: minio/minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - 9000:9000   # API
      - 9001:9001   # Console
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - iceberg_env

  # ---------------- Dremio ----------------
  dremio:
    image: dremio/dremio-oss:latest
    container_name: dremio
    ports:
      - 9047:9047   # Web UI
      - 31010:31010 # JDBC
      - 32010:32010 # Flight SQL
    networks:
      - iceberg_env

  # ---------------- Nessie ----------------
  nessie:
    image: projectnessie/nessie
    container_name: nessie
    ports:
      - 19120:19120
    volumes:
      - ./nessie_data:/var/lib/nessie
    environment:
      - NESSIE_VERSION_STORE_TYPE=JDBC
      - QUARKUS_DATASOURCE_DB_KIND=postgresql
      - QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://postgres:5432/nessie
      - QUARKUS_DATASOURCE_USERNAME=nessie_user
      - QUARKUS_DATASOURCE_PASSWORD=nessie_pass
      - QUARKUS_OPENTELEMETRY_TRACER_EXPORTER_OTLP_ENABLED=false

    depends_on:
      - postgres
    networks:
      - iceberg_env

  # ---------------- Postgres for Nessie ----------------
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      - POSTGRES_DB=nessie
      - POSTGRES_USER=nessie_user
      - POSTGRES_PASSWORD=nessie_pass
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    networks:
      - iceberg_env

  # ---------------- Superset ----------------
  dashboards:
    image: apache/superset:3.1.0
    container_name: dashboards
    environment:
      - SUPERSET_SECRET_KEY=60FTM+mZv0plqhUrWRQtDBBSymw6raRJV2CDmY2hu5AoGpa6ggTx3gHA
    ports:
      - 8088:8088   # Superset UI
    networks:
      - iceberg_env

  # ---------------- Airflow Initialization ----------------
  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    depends_on:
      - airflow-postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_pass@airflow-postgres:5432/airflow_db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: >
      bash -c "
        echo 'Waiting for Postgres...' &&
        until nc -z airflow-postgres 5432; do
          sleep 2;
          echo 'Postgres not ready yet...';
        done &&
        echo 'Postgres ready, initializing Airflow DB...' &&
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    networks:
      - iceberg_env

  # ---------------- Airflow ----------------
  airflow:
    image: apache/airflow:2.8.0
    container_name: airflow
    restart: always
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_pass@airflow-postgres:5432/airflow_db
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - 8081:8080
    command: >
      bash -c "
        airflow scheduler &
        exec airflow webserver
      "
    networks:
      - iceberg_env


  # ---------------- MLflow ----------------
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.1
    container_name: mlflow
    environment:
      - BACKEND_STORE_URI=sqlite:///mlflow.db
      - ARTIFACT_ROOT=/mlflow/mlruns
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root /mlflow/mlruns
      --host 0.0.0.0
      --port 5000
    ports:
      - 5000:5000
    volumes:
      - ./mlflow/mlruns:/mlflow/mlruns
    networks:
      - iceberg_env


  # ---------------- Postgres for Airflow ----------------
  airflow-postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      - POSTGRES_DB=airflow_db
      - POSTGRES_USER=airflow_user
      - POSTGRES_PASSWORD=airflow_pass
    volumes:
      - ./postgres_airflow_data:/var/lib/postgresql/data
    networks:
      - iceberg_env

# ---------------- Networks & Volumes ----------------
networks:
  iceberg_env:
    driver: bridge

volumes:
  minio_data:
